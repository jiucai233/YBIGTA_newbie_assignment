# 프롬프팅 기법별 성능 비교 및 분석 보고서

본 보고서는 GSM8K 데이터셋을 활용하여 Direct Prompting, Chain-of-Thought (CoT) Prompting, 그리고 My Prompting 기법의 성능을 비교 분석한 결과를 담고 있습니다.

## 1. 프롬프팅 기법별 정답률 비교

| 프롬프팅 기법 | 0-shot 정답률 | 3-shot 정답률 | 5-shot 정답률 |
| --- | --- | --- | --- |
| **Direct Prompting** | 84.00% | 74.00% | 74.00% |
| **CoT Prompting** | 70.00% | 74.00% | 84.00% |
| **My Prompting** | **88.00%** | **78.00%** | **84.00%** |

*참고: My Prompting 기법은 전반적으로 가장 높은 성능을 보였으며, 특히 0-shot 상황에서 88%의 높은 정답률을 기록하였습니다.*


## 2. CoT Prompting의 우수성 분석

Chain-of-Thought (CoT) Prompting이 Direct Prompting에 비해 복잡한 추론 문제에서 더 좋은 성능을 낼 수 있는 이유는 다음과 같습니다.

* **단계적 사고 과정의 가시화:** CoT는 문제를 한 번에 해결하려 하지 않고, 해결 과정을 작은 논리적 단계로 나눕니다. 이는 모델이 복잡한 계산이나 논리적 비약을 피하고 각 단계의 정확성을 높이는 데 도움을 줍니다.
* **오류 추적의 용이성:** 결론만 도출하는 Direct Prompting과 달리, CoT는 사고 과정을 함께 생성하기 때문에 모델이 스스로 논리적 일관성을 유지할 가능성이 높아집니다.

## 3. My Prompting의 우수성 분석

본 보고서에서 제안된 My Prompting 기법은 추론 과정에 **자가 검증(Self-Verification)** 메커니즘을 결합하여 일반적인 CoT보다 더 뛰어난 성능을 보입니다.

* **검증 단계의 추가:** CoT 기반 추론 이후, LLM이 자신의 답변을 다시 한번 검토하도록 유도하는 프롬프트를 삽입하였습니다. 이를 통해 초기 추론 과정에서 발생할 수 있는 사소한 산수 실수를 잡아낼 수 있어 정확도가 향상되었습니다.

## 4. 결론 및 시사점

실험 결과, 단순한 결론 도출보다는 사고 과정을 명시(CoT)하고, 이를 다시 검증하는 방식이 대규모 언어 모델의 수학적 추론 능력을 극대화하는 데 가장 효과적임을 확인하였습니다. 특히 My Prompting 기법은 0-shot에서도 강력한 성능을 보여, 예시 데이터가 부족한 환경에서도 효율적인 추론 도구로 활용될 수 있음을 시사합니다.

## 5. 개선점 및 고찰

* 데이터 오염 가능성: 현재 대부분의 LLM은 웹상에 공개된 데이터셋을 기반으로 사전 학습됩니다. 8B 규모의 소형 모델이 0-shot에서 84%라는 높은 수치를 기록한 점(공식 8-shot 기록인 80%를 상회)을 고려할 때, 해당 모델의 학습 데이터에 GSM8K 데이터셋이 포함되었을 가능성을 배제할 수 없습니다.  
* 추론 효율성 및 모델 성능의 한계: CoT 및 Few-shot 기법을 적용할 경우 입력 토큰(Input Token)이 길어지게 됩니다. 실험 과정에서 입력 토큰량이 증가함에 따라 추론 시간(Latency)이 급격히 증가하는 것을 관찰하였습니다. (평균 2초/iter에서 최대 12~30초/iter로 증가) 이는 8B 모델이 복잡한 추론 작업을 수행할 때 발생하는 성능상의 병목 현상으로 판단됩니다.  
* 주의집중 희석(Attention Dilution): Shot 수가 늘어남에도 성능이 정체되거나 하락하는 현상이 나타났습니다. 이는 소형 모델일수록 과도한 입력 토큰이 모델의 주의집중(Attention)을 분산시켜, 정답 도출에 필요한 핵심 정보에 집중하지 못하게 만들기 때문인 것으로 분석됩니다.  